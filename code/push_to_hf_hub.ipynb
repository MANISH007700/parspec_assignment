{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736d2d829ae041d29e1dc17c5d5a870f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install transformers huggingface_hub\n",
    "\n",
    "import os \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\n",
    "# load it locally first \n",
    "model = BertForSequenceClassification.from_pretrained(\"/mnt/g/Users/Manish/Desktop/interview_prep/parspec/parspec_assignment/code/finetuned_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/mnt/g/Users/Manish/Desktop/interview_prep/parspec/parspec_assignment/code/finetuned_model\")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully...\")\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/luci007/LightingData-Bert-Finetuned/commit/0d9d571e7c2c8c193040f61dc783f31410c5a238', commit_message='Upload tokenizer', commit_description='', oid='0d9d571e7c2c8c193040f61dc783f31410c5a238', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"luci007/LightingData-Bert-Finetuned\")\n",
    "tokenizer.push_to_hub(\"luci007/LightingData-Bert-Finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97137f5f300d4cc08abc47b8174f0ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf6c31a2c744fc09c8f0b9c9873b8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1569aeb1294959b122ab25445b5000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20751cb1b834c6ea0bdfaf30046f7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading from hub just for testing purpose\n",
    "\n",
    "model_try = BertForSequenceClassification.from_pretrained(\"luci007/LightingData-Bert-Finetuned\")\n",
    "tokenizer_try = BertTokenizer.from_pretrained('luci007/LightingData-Bert-Finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='luci007/LightingData-Bert-Finetuned', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/g/Users/Manish/Desktop/interview_prep/parspec/parspec_assignment/code/push_to_hf_hub.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/g/Users/Manish/Desktop/interview_prep/parspec/parspec_assignment/code/push_to_hf_hub.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mlighting solutions canless ez trim lamp based canless downlight durable metal baffle trim attached j box housing required installation compatible led gu , gu e lamps maximum lamp wattage w ic rated, direct contact insulation spring clips firmly secure fixture ceiling air tite pre installed gasket b quick connect pre installed push wiring connectors c slide n side pre installed tool less strain reliefs eliminate need additional non metallic cable connectors product features traditional housing features minus housing canless ez trim canless ez trim bridges gap traditional downlighting housing trim lamp canless led downlighting canless ez trim available apertures designed look like halo best selling trim, wb features durable metal trim, attached junction box, white baffles ultimate visual comfort options gu , gu , e lamp bases make canless ez trim compatible led lamps market shallow options available aperture sizes, making perfect solution x construction air tite, ic rated, course, housings required installation halo canless ez trim perfect solution customers looking value priced, canless solutions single family multi family residential applications hl rsmf round square mounting frame mf rc round mounting frame drywall collar cez gu wbicat cez e wbicat hl rsmf round square mounting frame mf rc round mounting frame drywall collar cez gu wbicat cez e wbicat cez gu wbicat cez e wbicat also compatible halo mounting frames cut hole wire fixture install fixture install lamp lamp included b c learn scan canless ez trim cez gu wbicat inch, shallow gu lamp cez gu wbicat inch, shallow gu lamp cez e wbicat inch, shallow e lamp cez gu wbicat inch gu lamp cez e wbicat inch e lamp cez gu wbicat inch, shallow gu lamp cez e wbicat inch, shallow e lamp catalog number description upc accessories mounting frame designer overlays cez gu wbicat canless ez trim, gu lamp base, white flange white baffle trim, ic rated, air tite cez gu wbicat canless ez trim, gu lamp base, white flange white baffle trim, ic rated, air tite hl rsmf round square mounting frame mf rc round mounting frame drywall collar cez e wbicat canless ez trim, e lamp base, white flange white baffle trim, ic rated, air tite cez gu wbicat canless ez trim, gu lamp base, white flange white baffle trim, ic rated, air tite hl rsmf round square mounting frame mf rc round mounting frame drywall collar cez ltrmwhwb lens overlay, white trim, white baffle cez ltrmwhbb lens overlay, white trim, black baffle cez ltrmbkbb lens overlay, black trim, black baffle cez ltrmsn lens overlay, satin nickel cez ltrmtbz lens overlay, tuscan bronze cez gu wbicat canless ez trim, shallow, gu lamp base, white flange white baffle trim, ic rated, air tite cez e wbicat canless ez trim, e lamp base, white flange white baffle trim, ic rated, air tite cez e wbicat canless ez trim, shallow, e lamp base, white flange white baffle trim, ic rated, air tite compatible lamps included fixture maximum lamp wattage w designer overlays compatible cez fixtures cez gu wbicat cez gu wbicat cez gu wbicat cez gu wbicat cez e wbicat cez e wbicat cez e wbicat mr led gu par led gu mr led gu led gu par led gu br led gu br led gu par l led gu par led gu r led gu led gu br led gu par l led gu par led gu r l led gu mr led e par led e led e par led e br led e br led e par l led e par led e r led e led e br led e par l led e par led e r l led e cez ltrmwhwb cez ltrmsn cez ltrmwhbb cez ltrmtbz cez ltrmbkbb wet location rated lens, perfect bathrooms soffits trademarks property respective owners product availability, specifications, compliances subject change without notice canada sales mclaughlin road mississauga, ontario l r b p f cooper lighting solutions highway south peachtree city, ga p wwwcooperlightingcom cooper lighting solutions rights reserved printed usa publication br july , pm lighting brands ametrix atlite corelite ephesus fail safe halo halo commercial invue io iris lumark lumi mcgraw edison metalux mws neo ray portfolio prentalux printed lighting rsa shaper streetworks sure lites telensa controls brands greengate fifth light intelligent lighting controls connected lighting systems smart spaces platform wavelinx\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/g/Users/Manish/Desktop/interview_prep/parspec/parspec_assignment/code/push_to_hf_hub.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(text, padding \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, truncation \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/g/Users/Manish/Desktop/interview_prep/parspec/parspec_assignment/code/push_to_hf_hub.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m outputs \u001b[39m=\u001b[39m model_1(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/g/Users/Manish/Desktop/interview_prep/parspec/parspec_assignment/code/push_to_hf_hub.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(outputs\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/envs/ml_dl/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/envs/ml_dl/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/envs/ml_dl/lib/python3.10/site-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    297\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    299\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    302\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "text = \"\"\"lighting solutions canless ez trim lamp based canless downlight durable metal baffle trim attached j box housing required installation compatible led gu , gu e lamps maximum lamp wattage w ic rated, direct contact insulation spring clips firmly secure fixture ceiling air tite pre installed gasket b quick connect pre installed push wiring connectors c slide n side pre installed tool less strain reliefs eliminate need additional non metallic cable connectors product features traditional housing features minus housing canless ez trim canless ez trim bridges gap traditional downlighting housing trim lamp canless led downlighting canless ez trim available apertures designed look like halo best selling trim, wb features durable metal trim, attached junction box, white baffles ultimate visual comfort options gu , gu , e lamp bases make canless ez trim compatible led lamps market shallow options available aperture sizes, making perfect solution x construction air tite, ic rated, course, housings required installation halo canless ez trim perfect solution customers looking value priced, canless solutions single family multi family residential applications hl rsmf round square mounting frame mf rc round mounting frame drywall collar cez gu wbicat cez e wbicat hl rsmf round square mounting frame mf rc round mounting frame drywall collar cez gu wbicat cez e wbicat cez gu wbicat cez e wbicat also compatible halo mounting frames cut hole wire fixture install fixture install lamp lamp included b c learn scan canless ez trim cez gu wbicat inch, shallow gu lamp cez gu wbicat inch, shallow gu lamp cez e wbicat inch, shallow e lamp cez gu wbicat inch gu lamp cez e wbicat inch e lamp cez gu wbicat inch, shallow gu lamp cez e wbicat inch, shallow e lamp catalog number description upc accessories mounting frame designer overlays cez gu wbicat canless ez trim, gu lamp base, white flange white baffle trim, ic rated, air tite cez gu wbicat canless ez trim, gu lamp base, white flange white baffle trim, ic rated, air tite hl rsmf round square mounting frame mf rc round mounting frame drywall collar cez e wbicat canless ez trim, e lamp base, white flange white baffle trim, ic rated, air tite cez gu wbicat canless ez trim, gu lamp base, white flange white baffle trim, ic rated, air tite hl rsmf round square mounting frame mf rc round mounting frame drywall collar cez ltrmwhwb lens overlay, white trim, white baffle cez ltrmwhbb lens overlay, white trim, black baffle cez ltrmbkbb lens overlay, black trim, black baffle cez ltrmsn lens overlay, satin nickel cez ltrmtbz lens overlay, tuscan bronze cez gu wbicat canless ez trim, shallow, gu lamp base, white flange white baffle trim, ic rated, air tite cez e wbicat canless ez trim, e lamp base, white flange white baffle trim, ic rated, air tite cez e wbicat canless ez trim, shallow, e lamp base, white flange white baffle trim, ic rated, air tite compatible lamps included fixture maximum lamp wattage w designer overlays compatible cez fixtures cez gu wbicat cez gu wbicat cez gu wbicat cez gu wbicat cez e wbicat cez e wbicat cez e wbicat mr led gu par led gu mr led gu led gu par led gu br led gu br led gu par l led gu par led gu r led gu led gu br led gu par l led gu par led gu r l led gu mr led e par led e led e par led e br led e br led e par l led e par led e r led e led e br led e par l led e par led e r l led e cez ltrmwhwb cez ltrmsn cez ltrmwhbb cez ltrmtbz cez ltrmbkbb wet location rated lens, perfect bathrooms soffits trademarks property respective owners product availability, specifications, compliances subject change without notice canada sales mclaughlin road mississauga, ontario l r b p f cooper lighting solutions highway south peachtree city, ga p wwwcooperlightingcom cooper lighting solutions rights reserved printed usa publication br july , pm lighting brands ametrix atlite corelite ephesus fail safe halo halo commercial invue io iris lumark lumi mcgraw edison metalux mws neo ray portfolio prentalux printed lighting rsa shaper streetworks sure lites telensa controls brands greengate fifth light intelligent lighting controls connected lighting systems smart spaces platform wavelinx\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(text, padding = True, truncation = True, return_tensors='pt').to('cuda')\n",
    "outputs = model_1(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "output_class = predictions.argmax().item()\n",
    "\n",
    "print(\"Pred class from model -> \", output_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
