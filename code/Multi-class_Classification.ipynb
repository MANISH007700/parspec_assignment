{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !pip install transformers datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25729,"status":"ok","timestamp":1625398158764,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"bixw-FrJ-6La","outputId":"850fe9c3-dc21-4036-bcde-b1b0b5c43034"},"outputs":[],"source":["import torch, os\n","import pandas as pd\n","from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast\n","from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1625398170258,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"x9RjPCc76GjE","outputId":"2956b1e5-7d27-47b9-f70e-dd32944408bc"},"outputs":[],"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_org = pd.read_csv(r\"/mnt/g/Users/Manish/Desktop/interview_prep/parspec/parspec_assignment/code/training_df_info.csv\")\n","df_org_test = pd.read_csv(r\"/mnt/g/Users/Manish/Desktop/interview_prep/parspec/parspec_assignment/code/testing_df_info.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# drop na \n","df_org.dropna(inplace = True)\n","df_org_test.dropna(inplace = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_org"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labels = df_org['target'].unique().tolist()\n","labels"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","## Why we need id2label and labe2ids in NLP Projects\n","\n","In NLP tasks, especially those involving classification problems, id2label and label2id dictionaries are used to map class labels (categories) to integer IDs and vice versa. These mappings are essential for various stages of the NLP pipeline, such as data preprocessing, model training, and evaluation.\n","\n","Data preprocessing: In order to feed text data into an NLP model, the text must first be tokenized and then converted into numerical values. Similarly, class labels must also be transformed into numerical representations. The label2id dictionary helps in converting the original class labels into integer IDs.\n","\n","Model training: NLP models usually output probability distributions over classes as their predictions. During training, the model's predictions are compared against the ground truth labels (which have been converted to integer IDs) to compute the loss and optimize the model parameters.\n","\n","Model evaluation and interpretation: Once the model has been trained, its predictions (in the form of integer IDs) need to be mapped back to their original class labels to make the results interpretable. The id2label dictionary is used to perform this conversion.\n","\n","\n","For `BertForSequenceClassification` model as well I need these exact mapping of id2labels and labels2id in dictionary form.\n","\n","### Hence, fefore you start training your model, create a map of the expected ids to their labels with id2label and label2id:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["label2id = {\"Yes\": 1, \"No\" : 0}\n","id2label = {1: \"Yes\", 0: \"No\"}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Create a new column to represent the categories in numerical form\n","\n","I need a 'label' column heading with numeric value else while running the epochs with `trainer.train()` I will get below error\n","\n","```\n","BertForSequenceClassification ValueError: The model did not return \n","\n","```\n","\n","\n","### In below I am doing it manually, but I could have also done it with pd.factorize() as below\n","\n","Pandas factorize method is used for encoding categorical variables as integers. It assigns a unique integer value to each distinct category in a given Series or DataFrame, effectively transforming non-numeric data into numeric values."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["21fbbe9bd6d94f308668ac1e4c2a51d0","f391632fced34067a6bc7f76034d8d38","d9bad87a40624bbe9a7b8e5021b6b878","6f869fe141f847a881532c74c5d8b269","35a5b5e6ff374b6a80e8677d695a5409","bed0bf19829f4957985091c7f308190c","43a1a07853414f1fbd41c1d7a7e2a7fd","66f31b78d9974a51bd28e01d073b72a7","8ed902a5a04e4bdaac8f961afe52b622","95a51d9d15734391a3895ac76b7a3676","395f10a793684087b6e3747154e17f8d","c905e0961cf84af2bec6a29ddc0d4237","b3652deb88d94a3e91bc0fa58b9dce54","94930151f87e495b974a7f40de2b1db4","4e66a8a8057e4232a3881aef3ac3afa9","a098193923044bfc89afdfa4faf8cc25"]},"executionInfo":{"elapsed":2705,"status":"ok","timestamp":1625398193149,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"r80yPEvU79rx","outputId":"542fee56-a4bc-44c4-814e-8dd3c29eeb7f"},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", max_length=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["eda88132d04243bcb499af9e82a789ba","b9f606782e924fdd9a22ee6039ef76f2","ce256db990954d9f9782eae9a248b40b","4557e0d687a44a8ab109109094f3ebd6","5db915425bac4d9caba753797e9faf29","6613484a4d0c477fb4ec5a3d4da81a73","d391715c96a84d078976a10fab20265a","0479af3dea114a4b86c98cd104325344","f8532b3abd7e4243a8fb0ade8e0540e7","1886bd7f86534fcea494bef48439e7db","85c4358f5a6e411fbff53e3dfe3eb88d","cfe8a0c3575a49b6b298262ed7acf76d","4622af58d24f4eea8f67f49364118ccd","b42f5e933f494ea69f3dbaac391b5e23","f92aca32755f45c3ba2b8cab79d44eed","dd7628d863794892b9b1fdd7c5e5df42"]},"executionInfo":{"elapsed":17953,"status":"ok","timestamp":1625398216241,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"L7mT5FEU2Jyr","outputId":"b9133277-8b83-47f4-83ee-bdaa1ac55a3f"},"outputs":[],"source":["model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id)\n","model.to(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Useful information and understanding you know about the model from the above output\n","\n","**Model components:** The model consists of several key components, such as:\n","\n","* Embeddings (word, position, and token type embeddings)\n","\n","* Encoder layers (12 layers in this case, each with self-attention, intermediate, and output components)\n","\n","* Layer normalization and dropout layers for regularization\n","\n","* GELU activation functions used in the intermediate layers\n","\n","**Model dimensions:**\n","\n","* Word embeddings: The model has an embedding size of 768 dimensions and a vocabulary size of 32,000 tokens.\n","\n","* Position embeddings: The model can handle input sequences of up to 512 tokens in length.\n","\n","* Encoder layers: The model has 12 encoder layers, each with a hidden size of 768 and an intermediate layer size of 3072.\n","\n","* Task-specific classification layer: The BertForSequenceClassification model is designed for sequence classification tasks. It takes the final hidden state of the [CLS] token and passes it through a linear layer and a softmax function to produce class probabilities. In this case, the model is configured with a custom number of labels (NUM_LABELS) and label mappings (id2label, label2id)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["--------------------------\n","\n","## Lets understand the flow of a raw input-text > through the pretrained BERT Model > and finally coming out on the other side of the model as a class prediction in the context of this task-specific fine-tuning.\n","\n","In BERT-based models like BertForSequenceClassification, the [CLS] token (short for \"classification\") serves as a special token that is prepended to the input sequence. It is designed to be used as an aggregate representation of the entire input sequence for classification tasks.\n","\n","Here's a step-by-step breakdown of how the [CLS] token is handled during fine-tuning for a specific classification task:\n","\n","Tokenization: During the pre-processing of the input text, the tokenizer inserts the [CLS] token at the beginning of the input sequence. For example, if the input text is \"This is a sample sentence.\", the tokenized input would look like: \"[CLS] This is a sample sentence.\"\n","\n","**Embeddings:** The tokenized input sequence, including the [CLS] token, is passed through the BERT model's embedding layers, which convert the tokens into continuous-valued word vectors.\n","\n","**Encoder layers:** The embedded input sequence is then processed through the BERT model's encoder layers, which consist of self-attention mechanisms and feed-forward neural networks. During this process, the model learns to capture the semantic and syntactic information present in the input sequence, as well as any relationships between the tokens.\n","\n","**Final hidden state of [CLS]:** At the end of the BERT model's encoder layers, each token has a corresponding hidden state vector. For the [CLS] token, its final hidden state is used as an aggregated representation of the entire input sequence. This vector is then passed to the task-specific classification layer.\n","\n","**Linear layer:** The final hidden state of the [CLS] token is fed into a linear layer, which maps the 768-dimensional vector (assuming the base BERT model) to a vector of size equal to the number of target classes. This is essentially a weight matrix multiplication followed by a bias term addition.\n","\n","**Softmax function:** The output of the linear layer is then passed through a softmax function, which converts the raw output values into class probabilities. The softmax function ensures that the sum of probabilities across all classes equals 1.\n","\n","**Prediction:** The class with the highest probability is chosen as the final prediction for the given input sequence.\n","\n","During task-specific fine-tuning, the model learns to adjust its weights and biases based on the training data and the target labels. This involves updating both the BERT model's pre-trained parameters and the task-specific classification layer's parameters through backpropagation and optimization techniques like gradient descent. This fine-tuning process allows the model to adapt to the specific classification task and improve its performance on the given dataset.\n","\n","-------------------\n","\n","## In the above task specific fine tuning do all of the weights of the pretrained BERT model gets modified during this finetuning process or only some of the weights get modified ?\n","\n","\n","During task-specific fine-tuning, all of the weights of the pre-trained BERT model are potentially subject to modification, including the weights in the embedding layers, the encoder layers, and the classification layer. However, the extent to which each weight is modified depends on the learning rate, the specific task, and the training data.\n","\n","In general, fine-tuning a pre-trained model like BERT involves updating its weights to better adapt to the target task. When fine-tuning begins, the initial weights of the model come from the pre-trained model, which has already learned general language representations from a large-scale unsupervised task (e.g., masked language modeling).\n","\n","During fine-tuning, the model is exposed to the particular task-specific training data and labels, and the weights are updated using backpropagation and gradient descent.\n","\n","Typically, the learning rate for fine-tuning is set to be smaller than the learning rate used during pre-training. This is because the pre-trained model already has a good understanding of language, and the fine-tuning process aims to make small, incremental adjustments to the weights to adapt the model to the specific task without losing the valuable general language knowledge."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZROZOxM9xi3l"},"source":["## Splitting df_org\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1625398219485,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"HM58CfgUZ0oK"},"outputs":[],"source":["SIZE= df_org.shape[0]\n","\n","train_texts= list(df_org.input_text)\n","test_texts= list(df_org_test.input_text)\n","\n","train_labels= list(df_org.target)\n","test_labels= list(df_org_test.target)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SIZE"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4406,"status":"ok","timestamp":1625398225432,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"snOgiQe2mbpx"},"outputs":[],"source":["train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_encodings.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DataLoader(Dataset):\n","    \"\"\"\n","    Custom Dataset class for handling tokenized text data and corresponding labels.\n","    Inherits from torch.utils.data.Dataset.\n","    \"\"\"\n","    def __init__(self, encodings, labels):\n","        \"\"\"\n","        Initializes the DataLoader class with encodings and labels.\n","\n","        Args:\n","            encodings (dict): A dictionary containing tokenized input text data\n","                              (e.g., 'input_ids', 'token_type_ids', 'attention_mask').\n","            labels (list): A list of integer labels for the input text data.\n","        \"\"\"\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns a dictionary containing tokenized data and the corresponding label for a given index.\n","\n","        Args:\n","            idx (int): The index of the data item to retrieve.\n","\n","        Returns:\n","            item (dict): A dictionary containing the tokenized data and the corresponding label.\n","        \"\"\"\n","        # Retrieve tokenized data for the given index\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        # Add the label for the given index to the item dictionary\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the number of data items in the dataset.\n","\n","        Returns:\n","            (int): The number of data items in the dataset.\n","        \"\"\"\n","        return len(self.labels)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## From above DataLoader() The line `item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}`\n","\n","Here I use a dictionary comprehension that constructs a new dictionary called `item`. This line converts the encoding values associated with the input text at the given index idx into PyTorch tensors.\n","\n","self.encodings is a dictionary containing tokenized input text with keys like 'input_ids', 'token_type_ids', and 'attention_mask'. These keys represent different aspects of the encoded text that are needed for processing by the BERT model. The values associated with these keys are lists or arrays of integers.\n","\n",".items() is a method that returns a view object displaying a list of a dictionary's key-value pairs as tuples.\n","\n","The dictionary comprehension iterates through the key-value pairs of self.encodings with the variables key and val. For each key-value pair, it creates a new key-value pair in the item dictionary, where the key remains the same, and the value is a PyTorch tensor created from the elements at index idx of the original value.\n","\n","In essence, this line of code is converting the relevant parts of the input encodings (e.g., input IDs, attention masks) at the given index idx into PyTorch tensors and storing them in a new dictionary called item. This format is necessary for input to the BERT model during training or evaluation.\n","\n","Here's an example of the output format for self.encodings:\n","\n","\n","```\n","{\n","    'input_ids': [\n","        [101, 2023, 2003, 1037, 2742, 102],\n","        [101, 1045, 2066, 5009, 2102, 102],\n","        [101, 2129, 2024, 2017, 1029, 102]\n","    ],\n","    'token_type_ids': [\n","        [0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0]\n","    ],\n","    'attention_mask': [\n","        [1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1]\n","    ]\n","}\n","```\n","\n","In this example, there are three input sentences, each encoded into three different features: input_ids, token_type_ids, and attention_mask.\n","\n","* input_ids: Lists of token IDs that represent the input text. The integers correspond to the tokens in the tokenizer's vocabulary.\n","\n","* token_type_ids: Lists of token type IDs that indicate the type of each token. In this case, they are all 0 since there is only one sentence per input. In tasks that require sentence pairs, you would see different token type IDs for different sentences.\n","\n","* attention_mask: Lists of binary values that indicate whether a given token should be attended to (1) or not (0). In this example, all tokens are attended to, so all the values are 1. Padding tokens would have a value of 0 in the attention_mask.\n","\n","Note that this example assumes that the maximum sequence length is 6 tokens, and there's no need for padding or truncation. In practice, you would have longer sequences, and padding would be necessary to make all the input sequences have the same length."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":412,"status":"ok","timestamp":1625398229008,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"Vx3u-9ljtmM_"},"outputs":[],"source":["train_dataloader = DataLoader(train_encodings, train_labels)\n","\n","test_dataset = DataLoader(test_encodings, test_labels)"]},{"cell_type":"markdown","metadata":{"id":"-dXcl4LWBsJy"},"source":["## Training with Trainer Class"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2440,"status":"ok","timestamp":1625398235958,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"jHP9LR_QsytZ"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","def compute_metrics(pred):\n","    \"\"\"\n","    Computes accuracy, F1, precision, and recall for a given set of predictions.\n","    \n","    Args:\n","        pred (obj): An object containing label_ids and predictions attributes.\n","            - label_ids (array-like): A 1D array of true class labels.\n","            - predictions (array-like): A 2D array where each row represents\n","              an observation, and each column represents the probability of \n","              that observation belonging to a certain class.\n","              \n","    Returns:\n","        dict: A dictionary containing the following metrics:\n","            - Accuracy (float): The proportion of correctly classified instances.\n","            - F1 (float): The macro F1 score, which is the harmonic mean of precision\n","              and recall. Macro averaging calculates the metric independently for\n","              each class and then takes the average.\n","            - Precision (float): The macro precision, which is the number of true\n","              positives divided by the sum of true positives and false positives.\n","            - Recall (float): The macro recall, which is the number of true positives\n","              divided by the sum of true positives and false negatives.\n","    \"\"\"\n","    # Extract true labels from the input object\n","    labels = pred.label_ids\n","    \n","    # Obtain predicted class labels by finding the column index with the maximum probability\n","    preds = pred.predictions.argmax(-1)\n","    \n","    # Compute macro precision, recall, and F1 score using sklearn's precision_recall_fscore_support function\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","    \n","    # Calculate the accuracy score using sklearn's accuracy_score function\n","    acc = accuracy_score(labels, preds)\n","    \n","    # Return the computed metrics as a dictionary\n","    return {\n","        'Accuracy': acc,\n","        'F1': f1,\n","        'Precision': precision,\n","        'Recall': recall\n","    }\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## compute_metrics\n","\n","argmax(): The NumPy method argmax() returns the index of the maximum value along a given axis. In a classification problem, we are interested in finding the class with the highest probability for each observation.\n","\n","(-1): The (-1) inside argmax() represents the axis along which the operation should be performed. In Python, negative indices are used to access elements from the end. So, (-1) here means the last axis, which is the columns in a 2D array."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1491,"status":"ok","timestamp":1625398239957,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"zMiPOIVAmYI2"},"outputs":[],"source":["training_args = TrainingArguments(\n","    # The output directory where the model predictions and checkpoints will be written\n","    output_dir='/mnt/g/Users/Manish/Desktop/interview_prep/parspec/parspec_assignment/model/finetune', \n","    do_train=True,\n","    do_eval=True,\n","    #  The number of epochs, defaults to 3.0 \n","    num_train_epochs=3,              \n","    per_device_train_batch_size=16,  \n","    per_device_eval_batch_size=32,\n","    # Number of steps used for a linear warmup\n","    warmup_steps=100,                \n","    weight_decay=0.01,\n","    logging_strategy='steps',\n","   # TensorBoard log directory                 \n","    logging_dir='./multi-class-logs',            \n","    logging_steps=50,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=50,\n","    save_strategy=\"steps\", \n","    fp16=False,\n","    load_best_model_at_end=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1625398241364,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"8Ajz99mwj-OL","outputId":"d3ee309d-d48a-4ed1-d9f6-ed48fcb3102f"},"outputs":[],"source":["trainer = Trainer(\n","    # the pre-trained model that will be fine-tuned \n","    model=model,\n","     # training arguments that we defined above                        \n","    args=training_args,                 \n","    train_dataset=train_dataloader,         \n","    eval_dataset=test_dataset,            \n","    compute_metrics= compute_metrics\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":701522,"status":"ok","timestamp":1625398945390,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"1WRhCH-Hj-RT","outputId":"fa531859-8922-4ced-9796-2301c3ebf091"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":315},"executionInfo":{"elapsed":85330,"status":"ok","timestamp":1625399043817,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"FN2i7kRIj-UQ","outputId":"71974f34-e461-4999-9ab8-558783167509"},"outputs":[],"source":["q=[trainer.evaluate(eval_dataset=df_org) for df_org in [train_dataloader, test_dataset]]\n","\n","pd.DataFrame(q, index=[\"train\",\"val\",\"test\"]).iloc[:,:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":392,"status":"ok","timestamp":1625399048192,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"I0guPT0jYJth"},"outputs":[],"source":["from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict(text):\n","    \"\"\"\n","    Predicts the class label for a given input text\n","\n","    Args:\n","        text (str): The input text for which the class label needs to be predicted.\n","\n","    Returns:\n","        probs (torch.Tensor): Class probabilities for the input text.\n","        pred_label_idx (torch.Tensor): The index of the predicted class label.\n","        pred_label (str): The predicted class label.\n","    \"\"\"\n","    # Tokenize the input text and move tensors to the GPU if available\n","    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n","\n","    # Get model output (logits)\n","    outputs = model(**inputs)\n","\n","    probs = outputs[0].softmax(1)\n","    \"\"\" Explanation outputs: The BERT model returns a tuple containing the output logits (and possibly other elements depending on the model configuration). In this case, the output logits are the first element in the tuple, which is why we access it using outputs[0].\n","\n","    outputs[0]: This is a tensor containing the raw output logits for each class. The shape of the tensor is (batch_size, num_classes) where batch_size is the number of input samples (in this case, 1, as we are predicting for a single input text) and num_classes is the number of target classes.\n","\n","    softmax(1): The softmax function is applied along dimension 1 (the class dimension) to convert the raw logits into class probabilities. Softmax normalizes the logits so that they sum to 1, making them interpretable as probabilities. \"\"\"\n","\n","    # Get the index of the class with the highest probability\n","    # argmax() finds the index of the maximum value in the tensor along a specified dimension.\n","    # By default, if no dimension is specified, it returns the index of the maximum value in the flattened tensor.\n","    pred_label_idx = probs.argmax()\n","\n","    # Now map the predicted class index to the actual class label \n","    # Since pred_label_idx is a tensor containing a single value (the predicted class index), \n","    # the .item() method is used to extract the value as a scalar\n","    pred_label = model.config.id2label[pred_label_idx.item()]\n","\n","    return probs, pred_label_idx, pred_label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1625399050044,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"mac8gGgWmYNn","outputId":"f6038697-d659-44b8-b0fe-946c9ce8e7ac"},"outputs":[],"source":["# Test with a an example text in Turkish\n","text = \"Makine öğrenimi kendisi de daha da otomatik hale doğru ilerliyor.\"\n","# \"Machine Learning itself is moving towards more and more automated\"\n","predict(text)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WWoJjzP6aYlc"},"source":["## Save model for inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2155,"status":"ok","timestamp":1625399060094,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"aF7GVSKSYBPw","outputId":"a0bcea98-9f37-460f-bf56-fa7604edd970"},"outputs":[],"source":["model_path = \"turkish-text-classification-model\"\n","trainer.save_model(model_path)\n","tokenizer.save_pretrained(model_path)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Re-Load saved model for inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2937,"status":"ok","timestamp":1625399066951,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"qEYQdBL8Tdkm","outputId":"0e6b93fa-32ab-481d-e676-94dd051acec6"},"outputs":[],"source":["model_path = \"turkish-text-classification-model\"\n","\n","\n","model = BertForSequenceClassification.from_pretrained(model_path)\n","tokenizer= BertTokenizerFast.from_pretrained(model_path)\n","nlp= pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":533,"status":"ok","timestamp":1625399077112,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"zyiJkIvzcdgC","outputId":"ddd71497-8990-4cde-95b9-99882f33bb17"},"outputs":[],"source":["nlp(\"Bugün hava çok güzel, dışarıda yürümek istiyorum.\")\n","# Today the weather is very nice, I want to go for a walk outside\n","\n","# Gives below output\n","#[{'label': 'saglik', 'score': 0.8295329213142395}]\n","# \"Saglik\" is a Turkish word that means \"health\" in English."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":515,"status":"ok","timestamp":1625399079590,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"RPLqmWPMGc53","outputId":"4070ccb8-9f2d-491e-b96e-0e8202777c07"},"outputs":[],"source":["nlp(\"Derin Öğrenme ve Yapay Zeka dünyayı değiştirecek.\")\n","# Deep Learning and AI is going to change the world\n","\n","# gives below output\n","#[{'label': 'teknoloji', 'score': 0.9932782053947449}]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":632,"status":"ok","timestamp":1625399082230,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"5hc65L_mQcVf","outputId":"43a2bda2-c915-47b5-f29c-8319bf76deeb"},"outputs":[],"source":["nlp(\"Son zamanlarda ekonomideki oynaklık nedeniyle, borsa endeksi oldukça düşük seviyelerde seyrediyor.\")\n","# Due to recent volatility in the economy, the stock market index has been at quite low levels\n","\n","#gives below output\n","#[{'label': 'ekonomi', 'score': 0.9850727915763855}]"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO2hBd/tE18yqAA78qNBNVu","collapsed_sections":[],"name":"CH05c_Multi-class_Classification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0479af3dea114a4b86c98cd104325344":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1886bd7f86534fcea494bef48439e7db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21fbbe9bd6d94f308668ac1e4c2a51d0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9bad87a40624bbe9a7b8e5021b6b878","IPY_MODEL_6f869fe141f847a881532c74c5d8b269"],"layout":"IPY_MODEL_f391632fced34067a6bc7f76034d8d38"}},"35a5b5e6ff374b6a80e8677d695a5409":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"395f10a793684087b6e3747154e17f8d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_94930151f87e495b974a7f40de2b1db4","max":59,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b3652deb88d94a3e91bc0fa58b9dce54","value":59}},"43a1a07853414f1fbd41c1d7a7e2a7fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4557e0d687a44a8ab109109094f3ebd6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0479af3dea114a4b86c98cd104325344","placeholder":"​","style":"IPY_MODEL_d391715c96a84d078976a10fab20265a","value":" 385/385 [00:34&lt;00:00, 11.2B/s]"}},"4622af58d24f4eea8f67f49364118ccd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"4e66a8a8057e4232a3881aef3ac3afa9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5db915425bac4d9caba753797e9faf29":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"6613484a4d0c477fb4ec5a3d4da81a73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66f31b78d9974a51bd28e01d073b72a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f869fe141f847a881532c74c5d8b269":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66f31b78d9974a51bd28e01d073b72a7","placeholder":"​","style":"IPY_MODEL_43a1a07853414f1fbd41c1d7a7e2a7fd","value":" 263k/263k [00:01&lt;00:00, 133kB/s]"}},"85c4358f5a6e411fbff53e3dfe3eb88d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_b42f5e933f494ea69f3dbaac391b5e23","max":445018749,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4622af58d24f4eea8f67f49364118ccd","value":445018749}},"8ed902a5a04e4bdaac8f961afe52b622":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_395f10a793684087b6e3747154e17f8d","IPY_MODEL_c905e0961cf84af2bec6a29ddc0d4237"],"layout":"IPY_MODEL_95a51d9d15734391a3895ac76b7a3676"}},"94930151f87e495b974a7f40de2b1db4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95a51d9d15734391a3895ac76b7a3676":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a098193923044bfc89afdfa4faf8cc25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3652deb88d94a3e91bc0fa58b9dce54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"b42f5e933f494ea69f3dbaac391b5e23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9f606782e924fdd9a22ee6039ef76f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bed0bf19829f4957985091c7f308190c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c905e0961cf84af2bec6a29ddc0d4237":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a098193923044bfc89afdfa4faf8cc25","placeholder":"​","style":"IPY_MODEL_4e66a8a8057e4232a3881aef3ac3afa9","value":" 59.0/59.0 [00:00&lt;00:00, 730B/s]"}},"ce256db990954d9f9782eae9a248b40b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_6613484a4d0c477fb4ec5a3d4da81a73","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5db915425bac4d9caba753797e9faf29","value":385}},"cfe8a0c3575a49b6b298262ed7acf76d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd7628d863794892b9b1fdd7c5e5df42","placeholder":"​","style":"IPY_MODEL_f92aca32755f45c3ba2b8cab79d44eed","value":" 445M/445M [00:09&lt;00:00, 49.2MB/s]"}},"d391715c96a84d078976a10fab20265a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9bad87a40624bbe9a7b8e5021b6b878":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_bed0bf19829f4957985091c7f308190c","max":262620,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35a5b5e6ff374b6a80e8677d695a5409","value":262620}},"dd7628d863794892b9b1fdd7c5e5df42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eda88132d04243bcb499af9e82a789ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce256db990954d9f9782eae9a248b40b","IPY_MODEL_4557e0d687a44a8ab109109094f3ebd6"],"layout":"IPY_MODEL_b9f606782e924fdd9a22ee6039ef76f2"}},"f391632fced34067a6bc7f76034d8d38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8532b3abd7e4243a8fb0ade8e0540e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85c4358f5a6e411fbff53e3dfe3eb88d","IPY_MODEL_cfe8a0c3575a49b6b298262ed7acf76d"],"layout":"IPY_MODEL_1886bd7f86534fcea494bef48439e7db"}},"f92aca32755f45c3ba2b8cab79d44eed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
